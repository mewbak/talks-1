# Introduction to concurrency & parallel programming
MCnet school, 2018-09

Sebastien Binet
CNRS/IN2P3/LPC
binet@cern.ch

## Disclaimer

I used to be a `FORTRAN` programmer (very briefly), then, a `C++` programmer, and after that, a `python` programmer.

Nowadays, I am a [Go](https://golang.org) programmer at heart, enjoying blazing fast compilation time, builtin concurrency constructs, easy installation and deployment, etc...

I really learned parallel/concurrent programming with `Go`.
My views on parallel programming with `C++ / Python` are thus somewhat skewed.

.image figs/gopher.png
The Go mascott: a gopher

## Parallel programming

Parallel programming is hard.
Parallel programming is hard, also in `Python`.
Parallel programming is harder in `C++`.

Actually, why do we have to do **parallel** programming ?

## Moore's law

.image figs/cpu-free-lunch.png _ 550

## The hardware/software contract

.image figs/par-prog-old-days.png _ 850

## 

.image figs/par-prog-power.png _ 850

## 

.image figs/par-prog-cores.png _ 850

## Free lunch is over

.image figs/head-on.png _ 900

## Raining transistors

  - Frequency has plateaued
  - **But** number of transistors is still following (more or less) Moore's Law

What can we do with them ?

## Hardware diversity: basic building blocks

.image figs/par-prog-building-blocks.png

## Hardware diversity: combining building blocks

.image figs/par-prog-heterogeneous.png

## Hardware diversity: CPUs

.image figs/par-prog-cpu.png _ 800

## Parallel programming

Ok, so we have to do parallel programming to properly utilize our new
hardware...

Actually, are we really sure we want to do **parallel** programming ?

## Concurrency vs Parallelism

_Concurrency:_ programming as the composition of independently executing processes/tasks.

_Parallelism:_ programming as the simultaneous execution of (possibly related) computations.

.image figs/conc-vs-par.png 350 _

## Interlude: Sequential, Concurrent & Parallel pizzas

## Pizza recipe

(**Disclaimer:** don't ever eat any pizza prepared or cooked by me.)

How to prepare a (sequential) pizza?

.code code/make-pizza.f

Estimated time (1 chef, 1 pizza):

	xx-oooo-xxx-oo-###

How to make this faster?

## (Sequential) Pizza recipe

Tasks:

  - wash tomatoes and onions
  - cut tomatoes, onions
  - prepare pizza dough
  - put tomato sauce on top of pizza dough
  - toppings: put tomatoes, onions, ham and mozarella
  - (pre-)heat oven, bake
  - (cut, then eat)

Estimated time (1 chef, 1 pizza):

	xx-oooo-xxx-oo-###

## Concurrent pizzas - Parallel pizzas

Estimated time (1 chef, 1 kitchen, 2 pizzas):

	xx-oooo-xxx-oo-###-xx-oooo-xxx-oo-###

Estimated time (1 chef, 2 kitchens, 2 pizzas):

	xx-oooo-xxx-oo+###
	              +xx-oooo-xxx-oo-###

Estimated time (2 chefs, 1 kitchen, 2 pizzas):

	xx-xxx-+-xx-xxx-+
	       +###     +###
	oooo-oo+-oooo-oo+

Estimated time (2 chefs, 2 kitchens, 2 pizzas):

	xx-oooo-xxx-oo-###
	xx-oooo-xxx-oo-###

## Concurrency vs Parallelism

Concurrency is about dealing with lots of things at once.
Parallelism is about doing lots of things at once.

Concurrency is about (program) **structure**.
Parallelism is about (program) **execution**.

.image figs/conc-vs-par-prog.png 350 _

## Concurrency vs Parallelism

Concurrency and parallelism are related.
Concurrency isn't parallelism (it's better!)

Parallelizing an application is done by:

  - finding concurrency in the problem
  - exposing the concurrency in the source code
  - exploiting the exposed concurrency to complete the job in less time.

.image figs/conc-vs-par-decomp.png

## Decomposition in parallel programs

Every parallel program is based on concurrency
i.e: tasks defined by an application that can run at the same time.

**EVERY** parallel program requires a _task decomposition_ and a _data decomposition_:

  - Task decomposition: break the application down into a set of tasks that can execute concurrently.
  - Data decomposition: How must the data be broken down into chunks and associated with threads/processes to make the parallel program run efficiently.

## Goldilocks

Parallel approaches have to find the "sweet spot" between two extremes.

Too fine grained:

  - Data: computation dominated by overhead
  - Tasks: context switching overhead

Too coarse grained:

  - Data: load balancing problems
  - Tasks: insufficient items to keep processes busy

## Amdhal's law

Amdahl's law can be formulated in the following way:

.html figs/amdhal-def.svg

where:

  - `S_latency` is the theoretical speedup of the execution of the whole task;
  - `s` is the **speedup** of the part of the task that benefits from improved system resources;
  - `p` is the **proportion** of execution time that the part benefiting from improved resources originally occupied.

.link https://en.wikipedia.org/wiki/Amdahl%27s_law

## 

.image figs/amdhal-law.png 500 _
.html figs/amdhal-def.svg

## Parallel programming: multi-process vs multi-threaded

Multi-process:

  - resource requirements are multiplied w/ nbr of process instances
  - (clever) use of `fork(2)` can mitigate this issue (but not in a `MT` environment)
  - one process can not corrupt the memory of another process
  - overhead of pushing data from one process to the other

Multi-threaded:

  - small context switch times (wrt an OS' process)
  - automatic sharing of many hardware resources (memory, fds, sockets...)
  - thread-safety of external libraries?
  - one thread can corrupt another thread

## Multi-threading & Multi-processing in a C++ world

Modern architectures impose massive challenges on programmability in the context of performance portability.

  - massive increase in on-node parallelism
  - deep memory hierarchies

Only **portable** parallelization solution for `C++` programmers (today?): **OpenMP** & **MPI**

  - hugely successful for years
  - widely used and supported
  - simple use for simple cases
  - very portable
  - highly optimized

_Which_ `C++` BTW ?

## C++ timeline

`C++` is a wide and complex language.
Know your `C++` and the (subset of?) `C++` you are **allowed** to write!

  - `C++03`?
  - `C++11`?
  - `C++14`?
  - `C++17`?
  - `C++2x`?

## 

.image figs/wg21-timeline-2018-06.png 600 _

## Parallelism in C++

`C++11` introduced lower level abstractions:

  - `std::thread`, `std::mutex`, `std::future`, ...
  - fairly limited, more is needed
  - `C++` needs stronger support for higher-level parallelism

Several proposals to the Standardization Committee are accepted or under consideration:

  - Technical Specification: Concurrency
  - Technical Specification: Parallelism
  - Other smaller proposals: resumable functions, task regions, executors

## Parallelism in C++

Currently, there is no overarching vision related to higher-level parallelism

  - goal is to standardize on a "big story" by 2020
  - no need for OpenMP, OpenACC, OpenCL, etc...

But for the moment, `C++` programmers are stuck with `C++11/14/17`...

## Interlude: memory model & tools

## Memory model

With `C++11`, finally `C++` has a memory model that contemplates a multi-threaded execution of a program.

A thread is a single flow of control within a program

  - Every thread can potentially access every object and function in the program
  - The interleaving of each thread's instructions is undefined

.image figs/thread-exec-race.png

## Memory model

`C++` guarantees that two threads can update and access **separate** memory locations without interfering with each other.

  - For all other situations updates and accesses have to be properly synchronized (_ie:_ atomics, locks, memory fences)
  - If updates and accesses to the same location by multiple threads are not properly synchronized, there is a data race (_ie:_ undefined behavior)
  - Data races can be made visible by transformations applied by the compiler or by the processor for performance reasons

## Tools to support parallel programming development/debugging

.link http://valgrind.org/docs/manual/drd-manual.html
.link http://valgrind.org/docs/manual/hg-manual.html
.link http://clang.llvm.org/docs/ThreadSanitizer.html

Detect races at runtime:

  - needs a test case and workload that triggers a race
  - no false positive, but can not detect all possible races

Needs a recompilation (`TSan`), (dramatic) increase of resources requirements at
runtime (CPU, VMem) b/c of code instrumentation and bookkeeping.

## C++ parallel programming: building blocks

## C++11 std::thread

`C++11` (finally!) standardized threads

  - `std::thread` is now part of the standard `C++` library
  - `std::thread` is an abstraction and maps to local platform threads (`POSIX`, `Windows(TM)`, ...)

## C++11 std::thread

.play code/hello-par.cxx.go  /START/,/STOP/

	$> g++ -o hello-par --std=c++11 -pthread hello-par.cxx
	$> ./hello-par
	** inside thread 139800850654976!

## C++11 std::thread - avoiding errors

.play code/hello-2.cxx.go  /START/,/STOP/

**(1)** Thread function must do **exception handling**: unhandled exception => program termination

**(2)** Must join with _thread_ **before** handle goes out of scope, otherwise:
program termination.

## Programming style

  - Old school: thread functions (what we just saw)
  - Middle school: function objects (functors)

.code code/par-functors.cxx

## Programming style

  - New school: `C++11` lambda functions (aka anonymous functions)

It's all about trade-offs...

**Lambda functions:**

  - easier and more readable (once your brained has been trained)
  - code remains inline
  - potentially more **dangerous** (`[&]` captures everything by reference)

**Functions:**

  - more efficient: lambdas involve class, function objects
  - potentially **safer**: requires explicit variable scoping
  - more cumbersome

## Example: matrix multiply

.image figs/matrix-multiply.png

## Sequential version

.image figs/matrix-multiply-seq.png

## Structured (fork-join) parallelism

A common pattern when creating multiple threads 

.image figs/matrix-multiply-fork-join.png

## Parallel solution

.image figs/matrix-multiply-par.png _ 850

## Types of parallelism

Most common types:

  - Data
  - Task
  - Embarrassingly parallel
  - Dataflow

## Data parallelism

.image figs/par-types-data.png

## Task parallelism

.image figs/par-types-task.png

## Embarrassingly parallel

.image figs/par-types-emb-par.png

## Dataflow

.image figs/par-types-dataflow.png

## C++ concurrency features

.image figs/cxx-features.png

## Futures

**Futures** provide a higher level of abstraction

  - you start an asynchronous/parallel operation
  - you are returned a handle to wait for the result
  - thread creation, join and exceptions are handled for you

## std::async + std::future

.image figs/future-async.png

## async operations

.image figs/future-async-2.png

## Concurrency and Parallelism in Python

## Python modules for Concurrency and Parallelism

  - [threading](https://docs.python.org/3/library/threading.html)
  - [multiprocessing](https://docs.python.org/3/library/multiprocessing.html)
  - [asyncio (Python 3)](https://docs.python.org/3/library/asyncio.html)

A rather good whirlwind tour about concurrency in Python is:

.link https://pymotw.com/3/concurrency.html

## Threading in Python

.link https://pymotw.com/3/threading/index.html
.play code/py-threading.go /START/,/STOP/

## Threading in Python (with arguments)

.play code/py-threading-args.go /START/,/STOP/

## 

.play code/py-threading-names.go /START/,/STOP/

## 

.play code/py-threading-with-lock.go /START/,/STOP/

## MT: perfs

.play code/py-gil-mt.go /START/,/STOP/

## 

.play code/py-gil-mt2.go /START/,/STOP/

## Multithreading in Python

`CPython` does not allow for pure-Python code to **really** execute in parallel: there is a [Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock)

The GIL is a mutex (mutually exclusive) that protects access to Python objects:

.play code/py-gil.go /START/,/STOP/

The GIL prevents multiple threads from executing Python bytecodes at once.
The GIL can thus degrade performances for multithreaded code.

.link http://www.dabeaz.com/python/GIL.pdf

## GIL & Multithreading

The GIL is a hindrance to actually leverage multicore performances.

Two exit strategies:

  - write a CPython C-extension module (in C): write multithreaded C/C++ code
  - instead of multiple threads, use multiple processes

## Mutiprocessing in Python

.link https://pymotw.com/3/multiprocessing/index.html
.play code/py-mp.go /START/,/STOP/

`worker` is actually executed in a different process.
The result is sent back to the mother process.

The `multiprocessing` API is mapped on the `threading` one.

## MP: perfs

.play code/py-mp-perf.go /START/,/STOP/

## 

.play code/py-mp-perf2.go /START/,/STOP/

## MultiProcessing

The `multiprocessing` module can increase performances _wrt_ `threading` (in CPython)

But:

  - data is being exchanged back and forth between processes
  - it can become a hindrance when that data becomes quite large
  - it is actually starting to become **distributed** programming (which is also quite a bit sophisticated and can become brittle)

## Coroutines in Python

Coroutines are similar to generator functions.

	Coroutines are computer-program components that generalize subroutines
	for non-preemptive multitasking, by allowing multiple entry points for
	suspending and resuming execution at certain locations. Coroutines are
	well-suited for implementing familiar program components such as
	cooperative tasks, exceptions, event loops, iterators, infinite lists
	and pipes.

.link https://en.wikipedia.org/wiki/Coroutine
.play code/py-coro.go /START/,/STOP/

## Coroutines and asyncio

.play code/py-asyncio.go /START/,/STOP/
.link https://pymotw.com/3/asyncio/index.html

`asyncio` uses a single-threaded, single-process approach in which parts of an application cooperate to switch tasks explicitly at optimal times.

## 

.play code/py-asyncio-chain.go /START/,/STOP/

## 

.play code/py-asyncio-wait.go /START/,/STOP/

## 

.play code/py-asyncio-prod-cons.go /START/,/STOP/

## Concurrency in Go

## Concurrency in Go

The [Go](https://golang.org) concurrency model is based on CSP (Tony Hoare, 1978):

.link https://en.wikipedia.org/wiki/Communicating_sequential_processes
.image figs/golang-logo.png 200 _

In Go, concurrency programming relies on two builtin tools:

  - goroutines: a function executing concurrently
  - channels: a typed conduit between goroutines, throught which data can be exchanged

## History

  - Project starts at Google in 2007 (by Griesemer, Pike, Thompson)
  - Open source release in November 2009
  - More than 1.1k contributors have joined the project
  - Version 1.0 release in March 2012
  - Version 1.1 release in May 2013
  - Version 1.2 release in December 2013
  - _[...]_
  - Version 1.8 release in February 2017
  - Version 1.9 release in August 2017
  - Version 1.10 release in February 2018
  - Version 1.11 release in August 2018

.link https://golang.org

## Elements of Go

  - Russ Cox, Robert Griesemer, Ian Lance Taylor, Rob Pike, Ken Thompson

  - **Concurrent**, **garbage-collected**
  - An Open-source general progamming language (BSD-3)
  - feel of a **dynamic** **language**: limited verbosity thanks to the _type_ _inference_ _system_, map, slices
  - safety of a **static** **type** **system**
  - compiled down to machine language (so it is fast, goal is ~10% of C)
  - **object-oriented** but w/o classes, **builtin** **reflection**
  - first-class functions with **closures**
  - implicitly satisfied **interfaces**

Available on all major platforms (`Linux`, `Windows`, `macOS`, `Android`, `iOS`, ...) and for many architectures (`amd64`, `arm`, `arm64`, `i386`, `s390x`, `mips64`, ...)

## Concurrency: basic examples

## A boring function

We need an example to show the interesting properties of the concurrency primitives.
To avoid distraction, we make it a boring example.

.play code/boring.go /START/,/STOP/

## Slightly less boring

Make the intervals between messages unpredictable (still under a second).

.code code/lessboring.go /START/,/STOP/

## Running it

The boring function runs on forever, like a boring party guest.

.play code/lessboring.go /START-PROG/,/STOP-PROG/

## Ignoring it

The `go` statement runs the function as usual, but doesn't make the caller wait.

It launches a goroutine.

The functionality is analogous to the `&` on the end of a shell command.

.play code/goboring.go 1,/^}/

## Ignoring it a little less

When `main` returns, the program exits and takes the boring function down with it.

We can hang around a little, and on the way show that both main and the launched goroutine are running.

.play code/waitgoboring.go /func.main/,/^}/

## Goroutines

What is a goroutine? It's an independently executing function, launched by a go statement.

It has its own call stack, which grows and shrinks as required.

It's very cheap. It's practical to have thousands, even hundreds of thousands of goroutines.

It's not a thread.

There might be only one thread in a program with thousands of goroutines.

Instead, goroutines are multiplexed dynamically onto threads as needed to keep all the goroutines running.

But if you think of it as a very cheap thread, you won't be far off.

## Communication

Our boring examples cheated: the main function couldn't see the output from the other goroutine.

It was just printed to the screen, where we pretended we saw a conversation.

Real conversations require communication.

## Channels

A channel in Go provides a connection between two goroutines, allowing them to communicate.

.code code/helpers.go /START1/,/STOP1/
.code code/helpers.go /START2/,/STOP2/
.code code/helpers.go /START3/,/STOP3/

## Using channels

A channel connects the main and boring goroutines so they can communicate.

.play code/changoboring.go /START1/,/STOP1/
.code code/changoboring.go /START2/,/STOP2/

## Synchronization

When the main function executes <–c, it will wait for a value to be sent.

Similarly, when the boring function executes c <– value, it waits for a receiver to be ready.

A sender and receiver must both be ready to play their part in the communication. Otherwise we wait until they are.

Thus channels both communicate and synchronize.

## The Go approach

Don't communicate by sharing memory, share memory by communicating.

## "Patterns"

## Generator: function that returns a channel

Channels are first-class values, just like strings or integers.

.play code/generatorboring.go /START1/,/STOP1/
.code code/generatorboring.go /START2/,/STOP2/

## Channels as a handle on a service

Our boring function returns a channel that lets us communicate with the boring service it provides.

We can have more instances of the service.

.play code/generator2boring.go /START1/,/STOP1/

## Multiplexing

These programs make Joe and Ann count in lockstep.
We can instead use a fan-in function to let whosoever is ready talk.

.code code/faninboring.go /START3/,/STOP3/
.play code/faninboring.go /START1/,/STOP1/

## Fan-in

.image figs/gophermegaphones.jpg

## Restoring sequencing

Send a channel on a channel, making goroutine wait its turn.

Receive all messages, then enable them again by sending on a private channel.

First we define a message type that contains a channel for the reply.

.code code/sequenceboring.go /START0/,/STOP0/

## Restoring sequencing.

Each speaker must wait for a go-ahead.

.code code/sequenceboring.go /START1/,/STOP1/
.code code/sequenceboring.go /START2/,/STOP2/
.play code/sequenceboring.go /START3/,/STOP3/

## Select

A control structure unique to concurrency.

The reason channels and goroutines are built into the language.

## Select

The select statement provides another way to handle multiple channels.
It's like a switch, but each case is a communication:

  - All channels are evaluated.
  - Selection blocks until one communication can proceed, which then does.
  - If multiple can proceed, select chooses pseudo-randomly.
  - A default clause, if present, executes immediately if no channel is ready.

.code code/select.go /START0/,/STOP0/

## Fan-in again

Rewrite our original fanIn function. Only one goroutine is needed. Old:

.code code/faninboring.go /START3/,/STOP3/

## Fan-in using select

Rewrite our original fanIn function. Only one goroutine is needed. New:

.play code/selectboring.go /START3/,/STOP3/

## Timeout using select

The time.After function returns a channel that blocks for the specified duration.
After the interval, the channel delivers the current time, once.

.play code/timeout.go /START1/,/STOP1/

## Timeout for whole conversation using select

Create the timer once, outside the loop, to time out the entire conversation.
(In the previous program, we had a timeout for each message.)

.play code/timeoutall.go /START1/,/STOP1/

## Quit channel

We can turn this around and tell Joe to stop when we're tired of listening to him.

.code code/quit.go /START1/,/STOP1/
.play code/quit.go /START2/,/STOP2/

## Receive on quit channel

How do we know it's finished? Wait for it to tell us it's done: receive on the quit channel

.code code/rcvquit.go /START1/,/STOP1/
.play code/rcvquit.go /START2/,/STOP2/

## Daisy-chain

.play code/daisy.go /func/,$

## Chinese whispers, gopher style

.image figs/gophereartrumpet.jpg

## Conclusions (about Go)

Goroutines and channels make it easy to express complex operations dealing with:

  - multiple inputs
  - multiple outputs
  - timeouts
  - failure

And they're fun to use.

## Conclusions (about C&P)

That's all for today.
Many more to cover, though:

  - SIMD
  - Thread Building Blocks (`Intel` `TBB`)
  - `FPGA`
  - (auto-)vectorization
  - `OpenCL / OpenMP / OpenACC`
  - HPX

  - dead locks, live locks
  - false sharing
  - mutexes, memory barriers, spin lock, ...
  - ...

## To recap

Remember the pizzas?

  - know your **resources** (# of chefs, \# of kitchens, ovens)
  - know the **tasks** (cut, pre-heat, bake) and their **dependencies**, composing the overall goal (pizza)
  - know the **latencies** (pre-heating) and the **bottlenecks** (oven) of your main task
  - infer/learn the **optimal** resources for the task(s) at hand (some cooks/kitchens are better at making pizzas, other: calzones)

(imperfect) analogy:

  - chef: thread of execution
  - kitchen: CPU Core, GPU, FPGA, ...
  - oven: disk, memory, i/o, ...
    

## Parallel programming: references

.link https://web2.infn.it/esc15
.link http://concur.rspace.googlecode.com/hg/talk/concur.html
.link http://sc13.supercomputing.org/sites/default/files/prog105/prog105.pdf
.link https://pymotw.com
