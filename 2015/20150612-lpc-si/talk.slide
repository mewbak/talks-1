# Concurrency in HEP
LPC-SI, 2015/06/12

Sebastien Binet
CNRS/IN2P3

## Ma vie, mon oeuvre

  - 2002-2006: Thesis in ATLAS, LPC-Clermont-Fd
  - 2006-2009: Post-doc in ATLAS, LBL-Berkeley
  - 2009-2011: Post-doc in ATLAS, LAL-Orsay
  - 2011-2015: IR, LAL-Orsay (ATLAS,LHCb,LSST)
  - 2015-....: IR, LPC-Clermont-Fd (LSST,Alice)

## ATLAS

## ATLAS detector (44m x 25m)

.image _figs/atlas-detector.gif 600 800

## Main steps of data analysis/massaging

  - **Generation:** production of a single physics event (_e.g.:_ a collision and its decay products)
  - **Simulation:** modelling interactions between particles and detector material
  - **Reconstruction:** building physics objects (electrons, photons, ...) from the detector signals (energy deposits)
  - **Analysis:** testing hypotheses against the reconstruction output

.image _figs/schema-general-flux-all-en.png 300 300

## Software in HEP - some numbers

An LHC experiment (_e.g._ ATLAS, CMS) is ~3000 physicists but only a
fraction of those is developing code.

Reconstruction frameworks grew from ~3M SLOC to ~5M

Summing over all HEP software stack for e.g. ATLAS:

  - event generators: ~1.4M SLOC (C++, FORTRAN-77)
  - I/O libraries ~1.7M SLOC (C++)
  - simulation libraries ~1.2M SLOC (C++)
  - reconstruction framework ~5M SLOC (C++) + steering/configuration (~1M SLOC python) (want to have a look at the [ATLAS code](http://acode-browser.usatlas.bnl.gov/lxr/source/)? [CMS code](https://github.com/cms-sw/cmssw)?)

**GCC:** ~7M SLOC

**Linux** **kernel** **3.6:** 15.9M SLOC

## Software development cycle

VCS (CVS, then SVN. GIT: not yet)

Nightlies (Jenkins or homegrown solution)

  - need a sizeable cluster of build machines (distcc, ccache, ...)
  - builds the framework stack in ~8h
  - produces ~2000 shared libraries
  - installs them on AFS (also creates RPMs and tarballs)

Devs can then test and develop off the nightly _via_ AFS

Every 6 months or so a new production release is cut, validated (then patched) and deployed on the World Wide LHC Computing Grid (WLCG).

Release size: **~5Gb**

  - binaries, libraries (externals+framework stack)
  - extra data (sqlite files, physics processes' modelisation data, ...)

## Software runtime ?

Big science, big data, big software, big numbers

  - ~1min to initialize the application
  - loading >500 shared libraries
  - connecting to databases (detector description, geometry, ...)
  - instantiating ~2000 C++ components
  - 2Gb/4Gb memory footprint per process

## ATLAS, General context

Data analysis for publication
- collect and massage raw data from detector (ADC, currents, ...)
- transform into physical quantities (energy deposits, positions, ...)
- transform into physics objects (electrons, photons, ...)

Reconstruction software: `Athena`

.image _figs/data-flux-summary-all.png 300 600

## Athena: a sophisticated framework

  - multi-language (`C++`, `python`, `FORTRAN`, `Java`)
  - ~300 recurrent developers,
  - ~1000 "one-time" developers
  - 2000 packages & 5000 components

.image _figs/athena-component-model.png 300 600

## Athena@ATLAS

Worked on `PyAthena`, a `CPython` interface layer to `Athena`:

  - write algorithms in `python`
  - faster to prototype, slower to execute

Worked on `perfmon`, a performance (`CPU`, `VMem`, `I/O`) monitor for `Athena`:

  - minimal overhead (compared to `cachegrind`)
  - "turn-key" solution for physicists
  - debug `C++` performance issues and memory leaks

.image _figs/fullmon-rdotoesd-perfmon.png 200 500

## Software development in HEP

  - `C++`: **slow** (very slow?) to compile/develop, **fast** to execute
  - `python`: **fast** development cycle (no compilation), **slow** to execute

.image _figs/xkcd-compiling.png 400 400

Are those our only options ?

## Moore's law

.image _figs/cpu-free-lunch.png 550 550

## Moore's law

  - Moore's law still observed at the hardware level
  - **However** the _effective_ perceived computing power is mitigated

_"Easy_ _life"_ during the last 20-30 years:

  - Moore's law translated into **doubling** compute capacity every ~18 months (_via_ clock frequency)
  - **Concurrency** and **parallelism** necessary to efficiently harness the compute power of our new multi-core CPU architectures.

_But_ our current software isn't prepared for parallel/concurrent environments.

## AthenaMP@ATLAS

Worked on `AthenaMP`: a multi-process/multi-core version of `Athena`.

  - master process `fork()` `n` sub-processes
  - shares memory _via_ `Copy-On-Write` (`COW`)
  - uses `n` cores of your multi-core machine

.image _figs/athena-mp-seq.png 300 600

## Interlude: concurrency & parallelism

## Interlude: concurrency & parallelism

  - **Concurrency** is about _dealing_ with lots of things at once.
  - **Parallelism** is about _doing_ lots of things at once.
  - Not the same, but related.
  - Concurrency is about _structure_, parallelism is about _execution_.

.image _figs/conc-para.png 200 600

Concurrency is a way to structure a program by breaking it into pieces that can be executed independently.
Communication is the means to coordinate the independent executions.

## Concurrency in HEP software

.image _figs/conc-level.png 600 400

## Concurrency in HEP software - II

.image _figs/levels-of-conc.png
.image _figs/gaudi-hive-2.png 250 350

## 

.image _figs/conc-para-mt-mp.png 600 1000

## Time for a new language ?

.image _figs/new-lang.png 600 800

## Candidates

  - python/pypy
  - FORTRAN-2008
  - Vala
  - Swift
  - Rust
  - Go
  - Chapel
  - Scala
  - Haskell
  - Clojure

## Why not Go ?

.play _code/hello.go

	$ go run hello.go
	Hello from Go

A nice language with a nice mascot.

.image _figs/golang-logo.png 200 400

## Go in a nutshell

[Go](https://golang.org) is a new, general-purpose programming language.

  - Compiled
  - Statically typed
  - Concurrent
  - Simple
  - Productive

"Go is a wise, clean, insightful, fresh thinking approach to the greatest-hits subset of the well understood."
- Michael T. Jones

## History

  - Project starts at Google in 2007 (by Griesemer, Pike, Thompson)
  - Open source release in November 2009
  - More than 250 contributors join the project
  - Version 1.0 release in March 2012
  - Version 1.1 release in May 2013
  - Version 1.2 release in December 2013
  - Version 1.3 release in June 2014
  - Version 1.4 release in December 2014

## Elements of Go

  - Founding fathers: Russ Cox, Robert Griesemer, Ian Lance Taylor, Rob Pike, Ken Thompson

  - Concurrent, garbage-collected
  - An Open-source general progamming language (BSD-3)
  - feel of a **dynamic** **language**: limited verbosity thanks to the _type_ _inference_ _system_, map, slices
  - safety of a **static** **type** **system**
  - compiled down to machine language (so it is fast, goal is ~10% of C)
  - **object-oriented** but w/o classes, **builtin** **reflection**
  - first-class functions with **closures**
  - implicitly satisfied **interfaces**

## Concurrency

## Goroutines

  - The _go_ statement launches a function call as a goroutine

	go f()
	go f(x, y, ...)

  - A goroutine runs concurrently (but not necessarily in parallel)
  - A goroutine has its own (growable/shrinkable) stack

## A simple example

.code _code/concurrency1.go /f START/,/f END/

Function f is launched as 3 different goroutines, all running concurrently:

.play _code/concurrency1.go /main START/,/main END/

## go in HEP

## go in HEP

  - Created an `Athena` like concurrent framework,
  - Built `fads` on top of it: a ["FAst Detector Simulation"](https://github.com/go-hep/fads) toolkit

Designed with concurrency in mind (`CPU` and `I/O`)

Results very encouraging:

  - Compared with a [ROOT](https://root.cern.ch) -based `C++` fast simulation tool ([Delphes](https://cp3.irmp.ucl.ac.be/projects/delphes))

## Results - testbenches

  - Linux: Intel(R) Core(TM)2 Duo CPU @ 2.53GHz, 4GB RAM, 2 cores
  - MacOSX-10.6: Intel(R) Xeon(R) CPU @ 2.27GHz, 172GB RAM, 16 cores
  - Linux: Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz, 100GB RAM, 40 cores

## Linux (40 cores) testbench: memory

.image _figs/lhcb3-rss.png 550 800

## Linux (40 cores) testbench: CPU

.image _figs/lhcb3-cpu.png 550 800

## Linux (40 cores) testbench: event throughput

.image _figs/lhcb3-hz.png 550 800

## Conclusions

[Go](https://golang.org) is really nice and can address the challenges of multicore machines.

Can't wait to see it take `HEP` (and astro/cosmo) by storm.

Working on providing useful software stacks for these communities:

  - [go-hep](https://go-hep.github.io)
  - [astrogo](https://github.com/astrogo)

[Go](https://golang.org) is also a nice platform for DevOps: [Docker](https://docker.io) and [CoreOS](https://coreos.com)

(_but_ _that's_ _for_ _next_ _time..._)

## Backup

## Communication via channels

A channel type specifies a channel value type (and possibly a communication direction):

	chan int
	chan<- string  // send-only channel
	<-chan T       // receive-only channel

A channel is a variable of channel type:

	var ch chan int
	ch := make(chan int)  // declare and initialize with newly made channel

A channel permits _sending_ and _receiving_ values:

	ch <- 1   // send value 1 on channel ch
	x = <-ch  // receive a value from channel ch (and assign to x)

Channel operations synchronize the communicating goroutines.

## Communicating goroutines

Each goroutine sends its results via channel ch:

.code _code/concurrency2.go /f START/,/f END/

The main goroutine receives (and prints) all results from the same channel:

.play _code/concurrency2.go /main START/,/main END/
